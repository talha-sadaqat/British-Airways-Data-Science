import requests
from bs4 import BeautifulSoup
import pandas as pd
import os

base_url = "https://www.airlinequality.com/airline-reviews/british-airways"
pages = 10
page_size = 100

reviews = []

for i in range(1, pages + 1):
    print(f"Scraping page {i}")

    url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"
    response = requests.get(url)
    content = response.content
    parsed_content = BeautifulSoup(content, 'html.parser')
    
    for para in parsed_content.find_all("div", {"class": "text_content"}):
        reviews.append(para.get_text())
    
    print(f"   ---> {len(reviews)} total reviews")

# Create a DataFrame
df = pd.DataFrame()
df["reviews"] = reviews

# Define the directory path
directory = 'data'

# Check if the directory exists, if not, create it
if not os.path.exists(directory):
    os.makedirs(directory)

# Save the DataFrame to a CSV file in the 'data' directory
csv_file_path = os.path.join(directory, 'BA_reviews.csv')
df.to_csv(csv_file_path, index=False)
